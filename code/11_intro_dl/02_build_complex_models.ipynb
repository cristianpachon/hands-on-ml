{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Complex Models Using the Functional API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One example of a nonsequential neural network is Wide & Deep neural network. It connects all or part of the inputs directly to the output layer. This architecture makes it possible for the NN to learn both deep partterns and simple rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = fetch_california_housing()\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = keras.layers.Input(shape=X_train.shape[1:])\n",
    "hidden1 = keras.layers.Dense(3, activation=\"relu\")(input_)\n",
    "hidden2 = keras.layers.Dense(3, activation=\"relu\")(hidden1)\n",
    "concat = keras.layers.Concatenate()([input_, hidden2])\n",
    "output = keras.layers.Dense(1)(hidden1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Model(inputs=[input_], outputs=[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"mean_squared_error\", optimizer=\"sgd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/20\n",
      "11610/11610 [==============================] - 1s 96us/sample - loss: 1887810397075150.5000 - val_loss: 91966433.1163\n",
      "Epoch 2/20\n",
      "11610/11610 [==============================] - 1s 77us/sample - loss: 6401085.9908 - val_loss: 40.4043\n",
      "Epoch 3/20\n",
      "11610/11610 [==============================] - 1s 76us/sample - loss: 4.0778 - val_loss: 1.3444\n",
      "Epoch 4/20\n",
      "11610/11610 [==============================] - 1s 76us/sample - loss: 1.3303 - val_loss: 1.3441\n",
      "Epoch 5/20\n",
      "11610/11610 [==============================] - 1s 77us/sample - loss: 1.3305 - val_loss: 1.3440\n",
      "Epoch 6/20\n",
      "11610/11610 [==============================] - 1s 76us/sample - loss: 1.3306 - val_loss: 1.3441\n",
      "Epoch 7/20\n",
      "11610/11610 [==============================] - 1s 78us/sample - loss: 1.3309 - val_loss: 1.3441\n",
      "Epoch 8/20\n",
      "11610/11610 [==============================] - 1s 77us/sample - loss: 1.3305 - val_loss: 1.3441\n",
      "Epoch 9/20\n",
      "11610/11610 [==============================] - 1s 76us/sample - loss: 1.3307 - val_loss: 1.3441\n",
      "Epoch 10/20\n",
      "11610/11610 [==============================] - 1s 76us/sample - loss: 1.3303 - val_loss: 1.3455\n",
      "Epoch 11/20\n",
      "11610/11610 [==============================] - 1s 78us/sample - loss: 1.3304 - val_loss: 1.3440\n",
      "Epoch 12/20\n",
      "11610/11610 [==============================] - 1s 77us/sample - loss: 1.3308 - val_loss: 1.3447\n",
      "Epoch 13/20\n",
      "11610/11610 [==============================] - 1s 77us/sample - loss: 1.3303 - val_loss: 1.3443\n",
      "Epoch 14/20\n",
      "11610/11610 [==============================] - 1s 76us/sample - loss: 1.3309 - val_loss: 1.3453\n",
      "Epoch 15/20\n",
      "11610/11610 [==============================] - 1s 75us/sample - loss: 1.3303 - val_loss: 1.3464\n",
      "Epoch 16/20\n",
      "11610/11610 [==============================] - 1s 75us/sample - loss: 1.3307 - val_loss: 1.3460\n",
      "Epoch 17/20\n",
      "11610/11610 [==============================] - 1s 75us/sample - loss: 1.3307 - val_loss: 1.3453\n",
      "Epoch 18/20\n",
      "11610/11610 [==============================] - 1s 78us/sample - loss: 1.3307 - val_loss: 1.3456\n",
      "Epoch 19/20\n",
      "11610/11610 [==============================] - 1s 77us/sample - loss: 1.3306 - val_loss: 1.3444\n",
      "Epoch 20/20\n",
      "11610/11610 [==============================] - 1s 73us/sample - loss: 1.3303 - val_loss: 1.3463\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=20,\n",
    "    validation_data=(X_valid, y_valid)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, what if we want to send a subset of features through the wide path and a different subset (possibly overlapping) through the deep path? In this case, one solution is to use multiple inputs. For example, suppose we want to send five features through the wide path (features 0 to 4) and six features through the deep path (features 2 to 7)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_A = keras.layers.Input(shape=[5], name=\"wide_input\")\n",
    "input_B = keras.layers.Input(shape=[6], name=\"deep_input\")\n",
    "hidden1 = keras.layers.Dense(2, activation=\"relu\")(input_B)\n",
    "hidden2 = keras.layers.Dense(2, activation=\"relu\")(hidden1)\n",
    "concat = keras.layers.concatenate([input_A, hidden2])\n",
    "output = keras.layers.Dense(1, name=\"output\")(concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Model(inputs=[input_A, input_B], outputs=[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=1e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_A, X_train_B = X_train[:, :5], X_train[:, 2:]\n",
    "X_valid_A, X_valid_B = X_valid[:, :5], X_valid[:, 2:]\n",
    "X_test_A, X_test_B = X_test[:, :5], X_test[:, 2:]\n",
    "X_new_A, X_new_B = X_test_A[:3], X_test_B[:, :3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/2\n",
      "11610/11610 [==============================] - 1s 102us/sample - loss: nan - val_loss: nan\n",
      "Epoch 2/2\n",
      "11610/11610 [==============================] - 1s 80us/sample - loss: nan - val_loss: nan\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    (X_train_A, X_train_B), y_train,\n",
    "    epochs=2,\n",
    "    validation_data=((X_valid_A, X_valid_B), y_valid)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m55",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m55"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
